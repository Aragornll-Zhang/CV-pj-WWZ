# -*- coding: utf-8 -*-
"""Alexnet修改_(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tMJrs5WESb9l8HGT6WeQslW3b98H6HXW

### 任务要求

以上project任选一个，组队形式提交源代码(Github repo地址)、训练模型、实验报告。详细的实验报告包括实验设置：数据集介绍，训练测试集划分，网络结构，batch size，learning rate，优化器，iteration，epoch，loss function，评价指标，结果可视化、tensorboard训练loss曲线、测试AP/Acc 曲线等等。
"""

#装载云盘
from google.colab import drive
drive.mount('/content/drive')

"""相比于LeNet，AlexNet有了以下进步：
1. Data Augmentation：主要有水平翻转，随机裁剪、平移变换，颜色、光照变化。

2. 使用了Relu激活函数

3. 使用了Dropout正则机制

4. LRN（临近数据的归一化）

5. overlapping Pooling。其实就是带有stride移动（非默认）的pooling

6. 多GPU（本次实现中未用）——不设置实验

对于模型的的计算，绘制training，lossfunction
"""

import pandas as pd
import numpy as np
import torch.utils.data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torch 
from torch.utils.data import DataLoader
from torch import optim
import torch.nn as nn
import torch.backends.cudnn as cudnn
import torchvision
from matplotlib import pyplot as plt
#from sklearn import datasets
from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix, precision_score, accuracy_score,recall_score, f1_score,roc_auc_score
from sklearn.metrics import roc_curve, auc
import torch.nn.functional as F

"""#### 对transform的图像进行可视化"""

from skimage import img_as_ubyte
import cv2
from google.colab.patches import cv2_imshow

transform = transforms.Compose([ transforms.ToTensor()])
test_dataset = datasets.CIFAR10( 'cifar',train=False, download=True,transform=transform)
test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)
# 得到一个随机的训练图片
dataiter = iter(test_loader)
for i in range(10):
  
  images, labels = dataiter.next()
  transform=transforms.Compose([transforms.Resize((224,224))])
  images_new=transform(images)
  images = torch.squeeze(images) 
  images = torch.transpose(images, 0, -1) 
  img = images.numpy() 
  img = img_as_ubyte(img) # 这点很重要！！这些数值都不是在0-255，所以要转换为unit8
  cv2.imwrite("/content/drive/MyDrive/cv_pj/transform_224/image%d.jpg"%i, img) # 保存为test.jpg
  #cv2_imshow(img)
  cv2.waitKey(0)

  images_new = torch.squeeze(images_new) 
  images_new = torch.transpose(images_new, 0, -1) 
  images_new = images_new.numpy() 
  images_new = img_as_ubyte(images_new) # 这点很重要！！这些数值都不是在0-255，所以要转换为unit8
  cv2.imwrite("/content/drive/MyDrive/cv_pj/transform_224/image_224_%d.jpg"%i, images_new) # 保存为test.jpg
  #cv2_imshow(images_new)
  cv2.waitKey(0)

print( "over.")

"""#### 对数据集增强方式可视化

"""

# Commented out IPython magic to ensure Python compatibility.
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import matplotlib.pyplot as plt


normalize = transforms.Normalize(mean=[0.49139968, 0.48215827, 0.44653124],
                                     std=[0.24703233, 0.24348505, 0.26158768])

train_dataloader = torch.utils.data.DataLoader(
        datasets.CIFAR10(root='cifar', train=True, transform=transforms.Compose([
            transforms.RandomHorizontalFlip(),
            transforms.RandomCrop(32, 4),
            transforms.ToTensor(),
            normalize,
        ]), download=True),
        batch_size=50, shuffle=True,
        num_workers=4, pin_memory=True)

dev_dataloader = torch.utils.data.DataLoader(
        datasets.CIFAR10(root='cifar', train=False, transform=transforms.Compose([
            transforms.ToTensor(),
            normalize,
        ])),
        batch_size=50, shuffle=False,
        num_workers=4, pin_memory=True)


classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

# functions to show an image
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))


# get some random training images
dataiter = iter(train_dataloader)
images, labels = dataiter.next()
plt.figure(figsize=(20,10)) 

# show images
imshow(torchvision.utils.make_grid(images[0:8,:,:]))
# print labels
print(' '.join('%15s' % classes[labels[j]] for j in range(8)))

"""### 实验0

#### 原Alexnet

**对数据集进行resize，不改写alexnet**

参数设置：
batch_size=128

model:alexnet

lr=0.001

criterion = nn.CrossEntropyLoss(size_average=True)

optimizer = optim.Adam(model.parameters(), lr=lr)

scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

epoch=90
"""

#其实就是等比放大了图像，但是可能有利于大的kernel捕捉图像信息
batch_size = 128
cifar_train = datasets.CIFAR10('cifar', True, transform=transforms.Compose([
        transforms.Resize((224,224)),
        transforms.ToTensor()
    ]), download=True)
train_dataloader = DataLoader(cifar_train,batch_size=batch_size,shuffle=True)
cifar_test = datasets.CIFAR10('cifar', False, transform=transforms.Compose([
        transforms.Resize((224,224)),
        transforms.ToTensor()
    ]), download=True)
dev_dataloader= DataLoader(cifar_test,batch_size=batch_size,shuffle=False)

class AlexNet(nn.Module):

    def __init__(self, num_classes: int = 10) -> None:
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

unchanged_trainingloss=[]
unchanged_valloss=[]
unchanged_valacc=[]
unchanged_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
best_accuracy=0.0
es=0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    unchanged_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    unchanged_valloss.append(total_loss/steps)
    unchanged_valacc.append(total_correct/total_data_num) 
    unchanged_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es = 0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp0_unchanged/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp0_unchanged/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

unchanged_trainingloss=pd.DataFrame(data=unchanged_trainingloss)
unchanged_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp0_unchanged/unchanged_trainingloss.csv',encoding='utf-8')

unchanged_valloss=pd.DataFrame(data=unchanged_valloss)
unchanged_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp0_unchanged/unchanged_valloss.csv',encoding='utf-8')

unchanged_valacc=pd.DataFrame(data=unchanged_valacc)
unchanged_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp0_unchanged/unchanged_valacc.csv',encoding='utf-8')

unchanged_valacc5=pd.DataFrame(data=unchanged_valacc5)
unchanged_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp0_unchanged/unchanged_valacc5.csv',encoding='utf-8')

"""##### 原alexnet可视化"""

trainingloss_1_1 = pd.read_csv('/content/drive/MyDrive/cv_pj/exp0_unchanged/unchanged_trainingloss.csv', encoding="utf-8")
valloss_1_1=pd.read_csv('/content/drive/MyDrive/cv_pj/exp0_unchanged/unchanged_valloss.csv', encoding="utf-8")
valacc_1_1=pd.read_csv('/content/drive/MyDrive/cv_pj/exp0_unchanged/unchanged_valacc.csv',encoding='utf-8')
valacc5_1_1=pd.read_csv('/content/drive/MyDrive/cv_pj/exp0_unchanged/unchanged_valacc5.csv',encoding='utf-8')

trainingloss_1_1

plt.plot(trainingloss_1_1['Unnamed: 0'], trainingloss_1_1['0'])
plt.plot(valloss_1_1['Unnamed: 0'], valloss_1_1['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)

plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp0_unchanged/loss.jpg')
plt.show()

plt.plot(valacc_1_1['Unnamed: 0'], valacc_1_1['0'])
plt.plot(valacc5_1_1['Unnamed: 0'], valacc5_1_1['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp0_unchanged/acc.jpg')
plt.show()

model= torch.load('/content/drive/MyDrive/cv_pj/exp0_unchanged/epoch_17_accuracy_0.753500')
model.to(device)

test_transform = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

"""#### 修改后alexnet

参数设置：
batch_size=128

model:alexnet

lr=0.001

criterion = nn.CrossEntropyLoss(size_average=True)

optimizer = optim.Adam(model.parameters(), lr=lr)

scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

epoch=90

对alexnet进行修改，让适应32*32的图片size
对于早期的模型也有所修改，去掉了LRN
"""

NUM_CLASSES = 10
#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

train_batch_size=128
test_batch_size=128
train_transform = transforms.Compose([ transforms.ToTensor()])
test_transform = transforms.Compose([transforms.ToTensor()])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/changed_baseline/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/changed_baseline/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/changed_baseline/changed_trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/changed_baseline/changed_valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/changed_baseline/changed_valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/changed_baseline/changed_valacc5.csv',encoding='utf-8')

"""##### 修改后的alexnet可视化

"""

trainingloss_1_1 = pd.read_csv('/content/drive/MyDrive/cv_pj/changed_baseline/changed_trainingloss.csv', encoding="utf-8")
valloss_1_1=pd.read_csv('/content/drive/MyDrive/cv_pj/changed_baseline/changed_valloss.csv', encoding="utf-8")
valacc_1_1=pd.read_csv('/content/drive/MyDrive/cv_pj/changed_baseline/changed_valacc.csv',encoding='utf-8')
valacc5_1_1=pd.read_csv('/content/drive/MyDrive/cv_pj/changed_baseline/changed_valacc5.csv',encoding='utf-8')



plt.plot(trainingloss_1_1['Unnamed: 0'], trainingloss_1_1['0'])
plt.plot(valloss_1_1['Unnamed: 0'], valloss_1_1['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 20, 1)

plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/changed_baseline/loss.jpg')
plt.show()

plt.plot(valacc_1_1['Unnamed: 0'], valacc_1_1['0'])
plt.plot(valacc5_1_1['Unnamed: 0'], valacc5_1_1['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 20, 1)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/changed_baseline/acc.jpg')
plt.show()

"""###实验一：验证不同学习率对网络的影响

#### learning rate: 0.001
"""

NUM_CLASSES = 10
#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp1_0.001/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp1_0.001/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp1_0.001/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp1_0.001/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp1_0.001/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp1_0.001/valacc5.csv',encoding='utf-8')

changed_trainingloss

"""#### learning rate: 0.0001"""

NUM_CLASSES = 10
#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.0001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp1_0.0001/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp1_0.0001/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp1_0.0001/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp1_0.0001/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp1_0.0001/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp1_0.0001/valacc5.csv',encoding='utf-8')

"""#### 采用mulsetplr 即先0.001 训练，训练10个周期以后，再0.0001训练"""

train_batch_size=128
test_batch_size=128
train_transform = transforms.Compose([ transforms.ToTensor()])
test_transform = transforms.Compose([transforms.ToTensor()])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

NUM_CLASSES = 10
#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.1)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp1_changerate/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp1_changerate/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp1_changerate/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp1_changerate/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp1_changerate/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp1_changerate/valacc5.csv',encoding='utf-8')

"""#### 对实验一进行结果可视化

#### 0.001可视化
"""

trainingloss_1_1 = pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_0.001/trainingloss.csv', encoding="utf-8")
valloss_1_1=pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_0.001/valloss.csv', encoding="utf-8")
valacc_1_1=pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_0.001/valacc.csv',encoding='utf-8')
valacc5_1_1=pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_0.001/valacc5.csv',encoding='utf-8')

trainingloss_1_1.columns.values.tolist()

"""##### loss fig

"""

plt.plot(trainingloss_1_1['Unnamed: 0'], trainingloss_1_1['0'])
plt.plot(valloss_1_1['Unnamed: 0'], valloss_1_1['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 20, 1)

plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_0.001/loss.jpg')
plt.show()

"""##### acc fig"""

plt.plot(valacc_1_1['Unnamed: 0'], valacc_1_1['0'])
plt.plot(valacc5_1_1['Unnamed: 0'], valacc5_1_1['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 20, 1)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_0.001/acc.jpg')
plt.show()

"""##### 模型可视化"""

model= torch.load("/content/drive/MyDrive/cv_pj/exp1_0.001/epoch_12_accuracy_0.704000")
model.to(device)

test_transform = transforms.Compose([transforms.ToTensor()])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_0.001/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""#### 0.0001可视化"""

trainingloss_1_2 = pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_0.0001/trainingloss.csv', encoding="utf-8")
valloss_1_2=pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_0.0001/valloss.csv', encoding="utf-8")
valacc_1_2=pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_0.0001/valacc.csv',encoding='utf-8')
valacc5_1_2=pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_0.0001/valacc5.csv',encoding='utf-8')

"""##### loss fig"""

plt.plot(trainingloss_1_2['Unnamed: 0'], trainingloss_1_2['0'])
plt.plot(valloss_1_2['Unnamed: 0'], valloss_1_2['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)

plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_0.0001/loss.jpg')
plt.show()

"""##### acc fig

"""

plt.plot(valacc_1_2['Unnamed: 0'], valacc_1_2['0'])
plt.plot(valacc5_1_2['Unnamed: 0'], valacc5_1_2['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_0.0001/acc.jpg')
plt.show()

"""##### 模型可视化"""

model= torch.load("/content/drive/MyDrive/cv_pj/exp1_0.0001/epoch_19_accuracy_0.706000")
model.to(device)

test_transform = transforms.Compose([transforms.ToTensor()])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_0.0001/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""#### changerate"""

trainingloss_1_3 = pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_changerate/trainingloss.csv',encoding='utf-8')
valloss_1_3=pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_changerate/valloss.csv',encoding='utf-8')
valacc_1_3=pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_changerate/valacc.csv',encoding='utf-8')
valacc5_1_3=pd.read_csv('/content/drive/MyDrive/cv_pj/exp1_changerate/valacc5.csv',encoding='utf-8')

"""##### loss"""

plt.plot(trainingloss_1_3['Unnamed: 0'], trainingloss_1_3['0'])
plt.plot(valloss_1_3['Unnamed: 0'], valloss_1_3['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 20, 1)

plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_changerate/loss.jpg')
plt.show()

"""##### acc

"""

plt.plot(valacc_1_3['Unnamed: 0'], valacc_1_3['0'])
plt.plot(valacc5_1_3['Unnamed: 0'], valacc5_1_3['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 20, 1)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_changerate/acc.jpg')
plt.show()

"""##### 模型可视化"""

model= torch.load("/content/drive/MyDrive/cv_pj/exp1_changerate/epoch_13_accuracy_0.706600")
model.to(device)

test_transform = transforms.Compose([transforms.ToTensor()])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_changerate/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""#### 实验一纵向比较
由于采用了early stopping，所以收敛速度可视化机制可能不够好看。此外，由于top5正确率太高，不方便比较，因此仅仅比较top1正确率

##### acc 比较
"""

plt.plot(valacc_1_1['Unnamed: 0'], valacc_1_1['0'])
plt.plot(valacc_1_2['Unnamed: 0'], valacc_1_2['0'])
plt.plot(valacc_1_3['Unnamed: 0'], valacc_1_3['0'])
#plt.plot(valacc5_1_3['Unnamed: 0'], valacc5_1_3['0'])
plt.legend(['0.001 acc', '0.0001 acc','changerate acc'], loc=4)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 30, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_fig/acc.jpg')
plt.show()

"""##### loss 比较"""

plt.plot(trainingloss_1_1['Unnamed: 0'], trainingloss_1_1['0'])
plt.plot(trainingloss_1_2['Unnamed: 0'], trainingloss_1_2['0'])
plt.plot(trainingloss_1_3['Unnamed: 0'], trainingloss_1_3['0'])
plt.legend(['0.001 loss', '0.0001 loss','changerate loss'], loc=1)
#plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 30, 5)

plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp1_fig/loss.jpg')
plt.show()

"""### 实验二：数据集增强

#### 数据集baseline
"""

train_batch_size=128
test_batch_size=128
#transforms.RandomHorizontalFlip(), 
train_transform = transforms.Compose([transforms.ToTensor()])
test_transform = transforms.Compose([transforms.ToTensor()])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

NUM_CLASSES = 10

#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp2_base/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp2_base/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp2_base/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp2_base/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp2_base/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp2_base/valacc5.csv',encoding='utf-8')

"""#####  可视化"""

trainingloss = pd.read_csv('/content/drive/MyDrive/cv_pj/exp2_base/trainingloss.csv',encoding='utf-8')
valloss=pd.read_csv('/content/drive/MyDrive/cv_pj/exp2_base/valloss.csv',encoding='utf-8')
valacc=pd.read_csv('/content/drive/MyDrive/cv_pj/exp2_base/valacc.csv',encoding='utf-8')
valacc5=pd.read_csv('/content/drive/MyDrive/cv_pj/exp2_base/valacc5.csv',encoding='utf-8')

plt.plot(trainingloss['Unnamed: 0'], trainingloss['0'])
plt.plot(valloss['Unnamed: 0'], valloss['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 20, 1)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp2_base/loss.jpg')
plt.show()

plt.plot(valacc['Unnamed: 0'], valacc['0'])
plt.plot(valacc5['Unnamed: 0'], valacc5['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 20, 1)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp2_base/acc.jpg')
plt.show()

model= torch.load("/content/drive/MyDrive/cv_pj/exp2_base/epoch_12_accuracy_0.700700")
model.to(device)

test_transform = transforms.Compose([transforms.ToTensor()])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp2_base/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""#### 进行数据集加强

对于训练集，padding=4为上下左右均填充 4 个 pixel，由32×32的尺寸变为40×40，之后进行任意的裁剪；接着以0.5的概率进行水平翻转。将图像转化为tensor对象，并进行正态分布标准化。
对于测试集，不进行增强操作，仅仅对图像转化为tensor对象，并进行正态分布标准化，标准化的值与训练集相同。



"""

train_batch_size=128
test_batch_size=128
cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(cifar_norm_mean, cifar_norm_std)])

test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

NUM_CLASSES = 10

#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp2_aug/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp2_aug/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp2_aug/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp2_aug/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp2_aug/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp2_aug/valacc5.csv',encoding='utf-8')

"""##### 可视化"""

trainingloss = pd.read_csv('/content/drive/MyDrive/cv_pj/exp2_aug/trainingloss.csv',encoding='utf-8')
valloss=pd.read_csv('/content/drive/MyDrive/cv_pj/exp2_aug/valloss.csv',encoding='utf-8')
valacc=pd.read_csv('/content/drive/MyDrive/cv_pj/exp2_aug/valacc.csv',encoding='utf-8')
valacc5=pd.read_csv('/content/drive/MyDrive/cv_pj/exp2_aug/valacc5.csv',encoding='utf-8')

plt.plot(trainingloss['Unnamed: 0'], trainingloss['0'])
plt.plot(valloss['Unnamed: 0'], valloss['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp2_aug/loss.jpg')
plt.show()

plt.plot(valacc['Unnamed: 0'], valacc['0'])
plt.plot(valacc5['Unnamed: 0'], valacc5['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp2_aug/acc.jpg')
plt.show()

model= torch.load("/content/drive/MyDrive/cv_pj/exp2_aug/epoch_17_accuracy_0.752900")
model.to(device)

cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp2_aug/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""### 实验三：加上LRN 

正确率估计不会有很大的提高，但是可以比对运行时间
和实验二的数据集增强对比同周期数运行时间

##### 实验
"""

train_batch_size=128
test_batch_size=128
cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(cifar_norm_mean, cifar_norm_std)])

test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

import torch.nn as nn
from torch.nn import functional as F
from torch.autograd import Variable

class LRN(nn.Module):
    def __init__(self, local_size=1, alpha=1.0, beta=0.75, ACROSS_CHANNELS=False):
        super(LRN, self).__init__()
        self.ACROSS_CHANNELS = ACROSS_CHANNELS
        if self.ACROSS_CHANNELS:
            self.average=nn.AvgPool3d(kernel_size=(local_size, 1, 1), #0.2.0_4会报错，需要在最新的分支上AvgPool3d才有padding参数
                    stride=1,
                    padding=(int((local_size-1.0)/2), 0, 0)) 
        else:
            self.average=nn.AvgPool2d(kernel_size=local_size,
                    stride=1,
                    padding=int((local_size-1.0)/2))
        self.alpha = alpha
        self.beta = beta
    
    
    def forward(self, x):
        if self.ACROSS_CHANNELS:
            div = x.pow(2).unsqueeze(1)
            div = self.average(div).squeeze(1)
            div = div.mul(self.alpha).add(1.0).pow(self.beta)#这里的1.0即为bias
        else:
            div = x.pow(2)
            div = self.average(div)
            div = div.mul(self.alpha).add(1.0).pow(self.beta)
        x = x.div(div)
        return x

NUM_CLASSES = 10
#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp3/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp3/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp3/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp3/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp3/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp3/valacc5.csv',encoding='utf-8')

"""##### 可视化"""

trainingloss = pd.read_csv('/content/drive/MyDrive/cv_pj/exp3/trainingloss.csv',encoding='utf-8')
valloss=pd.read_csv('/content/drive/MyDrive/cv_pj/exp3/valloss.csv',encoding='utf-8')
valacc=pd.read_csv('/content/drive/MyDrive/cv_pj/exp3/valacc.csv',encoding='utf-8')
valacc5=pd.read_csv('/content/drive/MyDrive/cv_pj/exp3/valacc5.csv',encoding='utf-8')

plt.plot(trainingloss['Unnamed: 0'], trainingloss['0'])
plt.plot(valloss['Unnamed: 0'], valloss['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 55, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp3/loss.jpg')
plt.show()

plt.plot(valacc['Unnamed: 0'], valacc['0'])
plt.plot(valacc5['Unnamed: 0'], valacc5['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 55, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp3/acc.jpg')
plt.show()

model= torch.load("/content/drive/MyDrive/cv_pj/exp3/epoch_46_accuracy_0.788900")
model.to(device)

cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp3/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""### 实验四：更换优化器

实验结果没有本质差别

### 实验五：改变Dropout

#### dropout=0.5
"""

train_batch_size=128
test_batch_size=128
cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(cifar_norm_mean, cifar_norm_std)])

test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

NUM_CLASSES = 10

#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp5_0.5/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp5_0.5/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/valacc5.csv',encoding='utf-8')

"""###### 可视化"""

trainingloss = pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/trainingloss.csv',encoding='utf-8')
valloss=pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/valloss.csv',encoding='utf-8')
valacc=pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/valacc.csv',encoding='utf-8')
valacc5=pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/valacc5.csv',encoding='utf-8')

plt.plot(trainingloss['Unnamed: 0'], trainingloss['0'])
plt.plot(valloss['Unnamed: 0'], valloss['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 45, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp5_0.5/loss.jpg')
plt.show()

plt.plot(valacc['Unnamed: 0'], valacc['0'])
plt.plot(valacc5['Unnamed: 0'], valacc5['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 45, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp5_0.5/acc.jpg')
plt.show()

"""###### 模型可视化"""

model= torch.load("/content/drive/MyDrive/cv_pj/exp5_0.5/epoch_33_accuracy_0.778600")
model.to(device)

cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp5_0.5/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""#### dropout=0.9

###### 实验
"""

train_batch_size=128
test_batch_size=128
cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(cifar_norm_mean, cifar_norm_std)])

test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

NUM_CLASSES = 10

#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.9),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.9),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp5_0.9/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp5_0.9/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/valacc5.csv',encoding='utf-8')

"""##### 可视化"""

trainingloss = pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/trainingloss.csv',encoding='utf-8')
valloss=pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/valloss.csv',encoding='utf-8')
valacc=pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/valacc.csv',encoding='utf-8')
valacc5=pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/valacc5.csv',encoding='utf-8')

plt.plot(trainingloss['Unnamed: 0'], trainingloss['0'])
plt.plot(valloss['Unnamed: 0'], valloss['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 45, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp5_0.9/loss.jpg')
plt.show()

plt.plot(valacc['Unnamed: 0'], valacc['0'])
plt.plot(valacc5['Unnamed: 0'], valacc5['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 45, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp5_0.9/acc.jpg')
plt.show()

"""##### 模型可视化"""

model= torch.load("/content/drive/MyDrive/cv_pj/exp5_0.9/epoch_40_accuracy_0.730800")
model.to(device)

cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp5_0.9/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""#### 比对图"""

valloss5 = pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/valloss.csv',encoding='utf-8')
valloss9=pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/valloss.csv',encoding='utf-8')
valacc5=pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.5/valacc.csv',encoding='utf-8')
valacc9=pd.read_csv('/content/drive/MyDrive/cv_pj/exp5_0.9/valacc.csv',encoding='utf-8')

plt.plot(valloss5['Unnamed: 0'], valloss5['0'])
plt.plot(valloss9['Unnamed: 0'], valloss9['0'])
#plt.plot(valacc_1_3['Unnamed: 0'], valacc_1_3['0'])
#plt.plot(valacc5_1_3['Unnamed: 0'], valacc5_1_3['0'])
plt.legend(['drop =0.5', 'drop=0.9'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 50, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp5_fig/loss.jpg')
plt.show()

plt.plot(valacc5['Unnamed: 0'], valacc5['0'])
plt.plot(valacc9['Unnamed: 0'], valacc9['0'])
#plt.plot(valacc_1_3['Unnamed: 0'], valacc_1_3['0'])
#plt.plot(valacc5_1_3['Unnamed: 0'], valacc5_1_3['0'])
plt.legend(['drop =0.5', 'drop=0.9'], loc=4)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 50, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp5_fig/acc.jpg')
plt.show()

"""### 实验六：尝试不同的初始化

由于官方的初始化方式，已经应用到合适的初始化方式。使用自定义初始化方式，容易梯度为0或梯度爆炸

### 实验七：加上BN层

注意，这个实验结果是最好的

##### 实验
"""

train_batch_size=128
test_batch_size=128
cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(cifar_norm_mean, cifar_norm_std)])

test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

NUM_CLASSES = 10

#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.BatchNorm2d(192),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.BatchNorm2d(384),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.9),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.9),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp7/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp7/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp7/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp7/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp7/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp7/valacc5.csv',encoding='utf-8')

"""##### 可视化"""

trainingloss = pd.read_csv('/content/drive/MyDrive/cv_pj/exp7/trainingloss.csv',encoding='utf-8')
valloss=pd.read_csv('/content/drive/MyDrive/cv_pj/exp7/valloss.csv',encoding='utf-8')
valacc=pd.read_csv('/content/drive/MyDrive/cv_pj/exp7/valacc.csv',encoding='utf-8')
valacc5=pd.read_csv('/content/drive/MyDrive/cv_pj/exp7/valacc5.csv',encoding='utf-8')

plt.plot(trainingloss['Unnamed: 0'], trainingloss['0'])
plt.plot(valloss['Unnamed: 0'], valloss['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 45, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp7/loss.jpg')
plt.show()

plt.plot(valacc['Unnamed: 0'], valacc['0'])
plt.plot(valacc5['Unnamed: 0'], valacc5['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 45, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp7/acc.jpg')
plt.show()

"""##### 最优模型可视化"""

model= torch.load("/content/drive/MyDrive/cv_pj/exp7/epoch_35_accuracy_0.828000")
model.to(device)

cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp8/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""### 实验八：使用global average pooling代替fc

##### 训练
"""

train_batch_size=128
test_batch_size=128
cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(cifar_norm_mean, cifar_norm_std)])

test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

NUM_CLASSES = 10

#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(256, 10,kernel_size=1),
            nn.AvgPool2d(2, stride=1)
                      
        )
      

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 10)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp8/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp8/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp8/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp8/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp8/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp8/valacc5.csv',encoding='utf-8')

"""##### 可视化"""

trainingloss = pd.read_csv('/content/drive/MyDrive/cv_pj/exp8/trainingloss.csv',encoding='utf-8')
valloss=pd.read_csv('/content/drive/MyDrive/cv_pj/exp8/valloss.csv',encoding='utf-8')
valacc=pd.read_csv('/content/drive/MyDrive/cv_pj/exp8/valacc.csv',encoding='utf-8')
valacc5=pd.read_csv('/content/drive/MyDrive/cv_pj/exp8/valacc5.csv',encoding='utf-8')

plt.plot(trainingloss['Unnamed: 0'], trainingloss['0'])
plt.plot(valloss['Unnamed: 0'], valloss['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp8/loss.jpg')
plt.show()

plt.plot(valacc['Unnamed: 0'], valacc['0'])
plt.plot(valacc5['Unnamed: 0'], valacc5['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp8/acc.jpg')
plt.show()

"""##### 最优模型可视化"""

model= torch.load('/content/drive/MyDrive/cv_pj/exp8/epoch_19_accuracy_0.802100')
model.to(device)

cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp8/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()

"""### 实验九：对模型的一点修改
用sebolck进行模型增强

"""

train_batch_size=128
test_batch_size=128
cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(cifar_norm_mean, cifar_norm_std)])

test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
train_set = torchvision.datasets.CIFAR10(root='cifar', train=True, download=True, transform=train_transform)
train_dataloader = torch.utils.data.DataLoader(dataset=train_set, batch_size=train_batch_size, shuffle=True)
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, batch_size=test_batch_size, shuffle=False)

class SEblock(nn.Module):
    def __init__(self,input_channel: int, r: int = 16):
        super(SEblock, self).__init__()
        self.avgPool_glb = nn.AdaptiveAvgPool2d((1,1)) # C * H * W -> C
        self.fc1 = nn.Linear(input_channel , input_channel//r) # 降维
        self.fc2 = nn.Linear( input_channel//r ,input_channel) # 生维
        return

    def forward(self,x):
        # x : [BatchSize * C * H * W]
        # squeeze
        z = self.avgPool_glb(x) # [BS * C ]
        # excitation
        z = z.squeeze(-1).squeeze(-1)
        z =F.relu(self.fc1(z))  # [BS * C/r]
        z = torch.sigmoid(self.fc2(z)) # [BS * C ]
        # rescaling
        output = torch.einsum('ajbc,aj->ajbc ', x, z) # [B , C , H, W]
        return output

NUM_CLASSES = 10
#we modifiesd the net to fit the datasize
class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            SEblock(64),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            SEblock(192),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = AlexNet(10)
model.to(device)

lr=0.001
criterion = nn.CrossEntropyLoss(size_average=True)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)

changed_trainingloss=[]
changed_valloss=[]
changed_valacc=[]
changed_valacc5=[]

# Commented out IPython magic to ensure Python compatibility.
import time
epoch=90
es=0.0
best_accuracy=0.0
start_time=time.time()
for i in range(epoch):
    model.train()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_data_num = len(train_dataloader.dataset)
    steps = 0.0
    #训练
    for batch in train_dataloader:
        steps+=1
        optimizer.zero_grad() 
        # 取数据
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        #_, outputs = torch.max(outputs.data, 1)
        optimizer.zero_grad()
        loss = criterion(outputs, labels).to(device)

        loss.backward()
        optimizer.step()  

        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()

        if steps%100==0:
            print("Epoch %d_%.3f%%:  Training average Loss: %f"
#                       %(i, steps * train_dataloader.batch_size*100/len(train_dataloader.dataset),total_loss/steps))  
    changed_trainingloss.append(total_loss/steps)
    #验证
    model.eval()
    total_loss=0.0
    accuracy=0.0
    total_correct=0.0
    total_correctk=0.0
    total_data_num = len(dev_dataloader.dataset)
    steps = 0.0    
    for batch in dev_dataloader:
        steps+=1
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
        outputs = model(inputs)
        loss = criterion(outputs, labels)  
        total_loss = total_loss + loss.item() 
        correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   .view(labels.size()) == labels).sum()
        total_correct = total_correct + correct.item()
        #print('accuracy:{}'.format(accuracy_score(labels, y_score)))
        #print('precision:{}'.format(precision_score(labels, y_score,average='micro')))
        #print('recall:{}'.format(recall_score(labels, y_score,average='micro')))
        #print('f1-score:{}'.format(f1_score(labels, y_score,average='micro'))

        maxk = max((1,5))
        yresize = labels.view(-1,1)
        _, pred = outputs.topk(maxk, 1, True, True)

        correctk = torch.eq(pred, yresize).sum()

        #correct = (torch.max(outputs, dim=1)[1]  #get the indices
                   #.view(labels.size()) == labels).sum()
        total_correctk = total_correctk + correctk.item()
        
    print("Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f"
#       %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  
    print("Epoch %d :  Verification 5 accuracy: %f%%,Total Time:%f"
#       %(i,  total_correctk*100/total_data_num,time.time()-start_time))  
    changed_valloss.append(total_loss/steps)
    changed_valacc.append(total_correct/total_data_num) 
    changed_valacc5.append(total_correctk/total_data_num) 
    if best_accuracy < total_correct/total_data_num :
        es=0
        best_accuracy =total_correct/total_data_num 
        torch.save(model,'/content/drive/MyDrive/cv_pj/exp9/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        print('Model is saved in /content/drive/MyDrive/cv_pj/exp9/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))
        #torch.cuda.empty_cache()
    else:
        es += 1
        print("Counter {} of 5".format(es))
        if es > 4:
            print("Early stopping with best_acc: ", best_accuracy, "and val_acc for this epoch: ", total_correct/total_data_num, "...")
            break

changed_trainingloss=pd.DataFrame(data=changed_trainingloss)
changed_trainingloss.to_csv('/content/drive/MyDrive/cv_pj/exp9/trainingloss.csv',encoding='utf-8')

changed_valloss=pd.DataFrame(data=changed_valloss)
changed_valloss.to_csv('/content/drive/MyDrive/cv_pj/exp9/valloss.csv',encoding='utf-8')

changed_valacc=pd.DataFrame(data=changed_valacc)
changed_valacc.to_csv('/content/drive/MyDrive/cv_pj/exp9/valacc.csv',encoding='utf-8')

changed_valacc5=pd.DataFrame(data=changed_valacc5)
changed_valacc5.to_csv('/content/drive/MyDrive/cv_pj/exp9/valacc5.csv',encoding='utf-8')

"""##### 对模型进行可视化

"""

trainingloss = pd.read_csv('/content/drive/MyDrive/cv_pj/exp9/trainingloss.csv',encoding='utf-8')
valloss=pd.read_csv('/content/drive/MyDrive/cv_pj/exp9/valloss.csv',encoding='utf-8')
valacc=pd.read_csv('/content/drive/MyDrive/cv_pj/exp9/valacc.csv',encoding='utf-8')
valacc5=pd.read_csv('/content/drive/MyDrive/cv_pj/exp9/valacc5.csv',encoding='utf-8')

plt.plot(trainingloss['Unnamed: 0'], trainingloss['0'])
plt.plot(valloss['Unnamed: 0'], valloss['0'])
plt.legend(['training loss', 'val loss'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('loss')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)

plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp9/loss.jpg')
plt.show()

plt.plot(valacc['Unnamed: 0'], valacc['0'])
plt.plot(valacc5['Unnamed: 0'], valacc5['0'])
plt.legend(['top1 acc', 'top5 acc'], loc=1)
#设置坐标轴名称
plt.xlabel('epoch')
plt.ylabel('acc')
#设置坐标轴刻度
my_x_ticks = np.arange(0, 25, 5)
plt.xticks(my_x_ticks)
#plt.title('exp1 0.001 loss') 
plt.savefig('/content/drive/MyDrive/cv_pj/exp9/acc.jpg')
plt.show()

"""##### 对模型进行检测"""

model= torch.load('/content/drive/MyDrive/cv_pj/exp9/epoch_22_accuracy_0.804900')
model.to(device)

cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)
cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)
test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cifar_norm_mean, cifar_norm_std)])
test_set = torchvision.datasets.CIFAR10(root='cifar', train=False, download=True, transform=test_transform)
dev_dataloader = torch.utils.data.DataLoader(dataset=test_set, shuffle=False, batch_size=10000)

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)
  _, pred = torch.max(outputs, 1)
  pred=pred.cpu().numpy()
  labels=labels.cpu().numpy()
  ma_f1 = f1_score(labels, pred, average='macro')
  mi_f1 = f1_score(labels, pred, average='micro')
  print(ma_f1, mi_f1)
  print('accuracy:{}'.format(accuracy_score(labels, pred)))
  print('precision:{}'.format(precision_score(labels, pred,average='micro')))
  print('recall:{}'.format(recall_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='micro')))
  print('f1-score:{}'.format(f1_score(labels, pred,average='macro')))

for batch in dev_dataloader:
  steps+=1
  inputs, labels = batch
  inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU
  outputs = model(inputs)


scores = torch.softmax(outputs, dim=1).detach().cpu().numpy() # out = model(data)
labels=labels.cpu().numpy()
binary_label = label_binarize(labels, classes=list(range(10))) # num_classes=10

fpr = {}
tpr = {}
roc_auc = {}

for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(binary_label[:, i], scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(binary_label.ravel(), scores.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(10):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= 10
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(figsize=(8, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(10):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.grid()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.savefig('/content/drive/MyDrive/cv_pj/exp9/Multi-class ROC.jpg', bbox_inches='tight')
plt.show()